This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-12T01:34:17.766Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
listhost/
  listhost_signup.ipynb
  scrape_email.ipynb
scrape_db/
  blueprint_db_pipeline.ipynb
  categorize_rso.ipynb
  scrape_blueprint_all.ipynb
  scrape_blueprint_ind.ipynb
.gitignore
chatbot1.ipynb
README.md

================================================================
Files
================================================================

================
File: listhost/listhost_signup.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for listhost signup -- have to sign into listhost, then interact with page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from typing import Optional, Dict\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../login.json', 'r') as file:\n",
    "    login = json.load(file)\n",
    "\n",
    "# Access the credentials\n",
    "email = login['email']\n",
    "password = login['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='listhost_login.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListhostLogin:\n",
    "    def __init__(self, email: str, password: str):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        \n",
    "    def login(self) -> bool:\n",
    "        \"\"\"Handle the complete login process\"\"\"\n",
    "        try:\n",
    "            # 1. Navigate to the main page\n",
    "            logging.info(\"Navigating to main page...\")\n",
    "            self.driver.get(\"https://lists.uchicago.edu/web\")\n",
    "            time.sleep(2)  # Wait for page to fully load\n",
    "            \n",
    "            # 2. Click the initial login button\n",
    "            logging.info(\"Clicking initial login button...\")\n",
    "            login_button = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.NAME, \"action_login\"))\n",
    "            )\n",
    "            login_button.click()\n",
    "            time.sleep(1.5)  # Wait for transition\n",
    "            \n",
    "            # Wait for page transition (checking for email field presence)\n",
    "            email_field = self.wait.until(\n",
    "                EC.presence_of_element_located((By.ID, \"email_login\"))\n",
    "            )\n",
    "            \n",
    "            # 3. Fill in credentials\n",
    "            logging.info(\"Filling credentials...\")\n",
    "            email_field.send_keys(self.email)\n",
    "            time.sleep(0.5)  # Small wait between fields\n",
    "            \n",
    "            password_field = self.driver.find_element(By.ID, \"passwd\")\n",
    "            password_field.send_keys(self.password)\n",
    "            time.sleep(0.5)  # Wait before clicking submit\n",
    "            \n",
    "            # 4. Click the login submit button\n",
    "            logging.info(\"Submitting login form...\")\n",
    "            submit_button = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                \"button[class='heavyWork'][type='submit'][name='action_login']\"\n",
    "            )\n",
    "            submit_button.click()\n",
    "            time.sleep(2)  # Wait for login processing\n",
    "            \n",
    "            # 5. Wait for successful login (checking for email element)\n",
    "            self.wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, f'li[aria-label*=\"{self.email}\"]')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Successfully logged in!\")\n",
    "            time.sleep(1)  # Final wait to ensure everything is loaded\n",
    "            return True\n",
    "            \n",
    "        except TimeoutException as e:\n",
    "            logging.error(f\"Timeout while waiting for element: {str(e)}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during login process: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def is_logged_in(self) -> bool:\n",
    "        \"\"\"Check if we're currently logged in\"\"\"\n",
    "        try:\n",
    "            email_element = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                f'li[aria-label*=\"{self.email}\"]'\n",
    "            )\n",
    "            return email_element is not None\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example\n",
    "def test_login(email: str, password: str):\n",
    "    login_handler = ListhostLogin(email, password)\n",
    "    try:\n",
    "        success = login_handler.login()\n",
    "        if success:\n",
    "            print(\"Login successful!\")\n",
    "            # Here we could continue with list subscription logic\n",
    "        else:\n",
    "            print(\"Login failed!\")\n",
    "    finally:\n",
    "        login_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_login(email, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now making full scraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListhostSubscriber(ListhostLogin):\n",
    "    def __init__(self, email: str, password: str, max_subscriptions: int = 5):\n",
    "        super().__init__(email, password)\n",
    "        self.max_subscriptions = max_subscriptions\n",
    "        self.subscription_log_path = 'subscription_log.json'\n",
    "        self.subscription_attempts = self.load_subscription_log()\n",
    "        \n",
    "        # Track RSO subscription statuses\n",
    "        self.subscribed_lists = []\n",
    "        self.failed_lists = []\n",
    "        self.pending_lists = []\n",
    "        \n",
    "        # Set up logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('listhost_subscriber.log'),\n",
    "                logging.StreamHandler()  # This will print to console as well\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def load_rso_data(self, rso_data_path: str) -> None:\n",
    "        \"\"\"Load RSO data and categorize listhosts based on previous attempts\"\"\"\n",
    "        logging.info(f\"Loading RSO data from: {rso_data_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(rso_data_path, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logging.info(f\"Successfully loaded {len(rsos)} RSOs from file\")\n",
    "            \n",
    "            # Reset tracking lists\n",
    "            self.subscribed_lists = []\n",
    "            self.failed_lists = []\n",
    "            self.pending_lists = []\n",
    "            \n",
    "            valid_listhost_count = 0\n",
    "            invalid_listhost_count = 0\n",
    "            \n",
    "            for rso in rsos:\n",
    "                rso_name = rso.get('name', 'Unknown RSO')\n",
    "                logging.info(f\"\\nProcessing RSO: {rso_name}\")\n",
    "                \n",
    "                # Extract listhost email\n",
    "                listhost_email = rso.get('additional_info', {}).get('RSO Listhost')\n",
    "                if not listhost_email:\n",
    "                    logging.warning(f\"No listhost email found for: {rso_name}\")\n",
    "                    invalid_listhost_count += 1\n",
    "                    continue\n",
    "                \n",
    "                logging.info(f\"Found listhost email: {listhost_email}\")\n",
    "                \n",
    "                # Extract listhost name\n",
    "                listhost_name = self.extract_listhost_name(listhost_email)\n",
    "                if not listhost_name:\n",
    "                    logging.warning(f\"Could not extract valid listhost name from: {listhost_email}\")\n",
    "                    invalid_listhost_count += 1\n",
    "                    continue\n",
    "                \n",
    "                valid_listhost_count += 1\n",
    "                \n",
    "                # Create RSO info dictionary\n",
    "                rso_info = {\n",
    "                    'name': rso_name,\n",
    "                    'listhost': listhost_name,\n",
    "                    'email': listhost_email\n",
    "                }\n",
    "                \n",
    "                # Categorize based on previous attempts\n",
    "                if listhost_name in self.subscription_attempts:\n",
    "                    status = self.subscription_attempts[listhost_name]['status']\n",
    "                    logging.info(f\"Previous attempt found for {listhost_name} - Status: {status}\")\n",
    "                    \n",
    "                    if status == 'success' or status == 'already_subscribed':\n",
    "                        self.subscribed_lists.append(rso_info)\n",
    "                    else:\n",
    "                        self.failed_lists.append({**rso_info, 'status': status})\n",
    "                else:\n",
    "                    logging.info(f\"No previous attempts for {listhost_name} - Adding to pending\")\n",
    "                    self.pending_lists.append(rso_info)\n",
    "            \n",
    "            logging.info(\"\\nRSO Data Loading Summary:\")\n",
    "            logging.info(f\"Total RSOs processed: {len(rsos)}\")\n",
    "            logging.info(f\"Valid listhosts found: {valid_listhost_count}\")\n",
    "            logging.info(f\"Invalid/missing listhosts: {invalid_listhost_count}\")\n",
    "            logging.info(f\"Currently subscribed: {len(self.subscribed_lists)}\")\n",
    "            logging.info(f\"Previously failed: {len(self.failed_lists)}\")\n",
    "            logging.info(f\"Pending subscription: {len(self.pending_lists)}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"RSO data file not found: {rso_data_path}\")\n",
    "            raise\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"Invalid JSON format in file: {rso_data_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error loading RSO data: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def get_subscription_status(self) -> dict:\n",
    "        \"\"\"Return current subscription status counts\"\"\"\n",
    "        return {\n",
    "            'subscribed': len(self.subscribed_lists),\n",
    "            'failed': len(self.failed_lists),\n",
    "            'pending': len(self.pending_lists)\n",
    "        }\n",
    "        \n",
    "    def process_pending_batch(self):\n",
    "        \"\"\"\n",
    "        Process a batch of pending subscriptions up to max_subscriptions\n",
    "        \"\"\"\n",
    "        subscriptions_processed = 0\n",
    "        \n",
    "        for rso in self.pending_lists[:self.max_subscriptions]:\n",
    "            if subscriptions_processed >= self.max_subscriptions:\n",
    "                break\n",
    "                \n",
    "            if self.subscribe_to_listhost(rso['listhost']):\n",
    "                subscriptions_processed += 1\n",
    "                self.pending_lists.remove(rso)\n",
    "                self.subscribed_lists.append(rso)\n",
    "            else:\n",
    "                self.pending_lists.remove(rso)\n",
    "                self.failed_lists.append(rso)\n",
    "                \n",
    "        logging.info(f\"Batch complete. Processed {subscriptions_processed} subscriptions\")\n",
    "        \n",
    "    def load_subscription_log(self) -> Dict:\n",
    "        \"\"\"Load or create subscription attempt log\"\"\"\n",
    "        try:\n",
    "            with open(self.subscription_log_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "            \n",
    "    def save_subscription_log(self):\n",
    "        \"\"\"Save current subscription attempts\"\"\"\n",
    "        with open(self.subscription_log_path, 'w') as f:\n",
    "            json.dump(self.subscription_attempts, f, indent=2)\n",
    "            \n",
    "    def extract_listhost_name(self, email: str) -> Optional[str]:\n",
    "        \"\"\"Extract listhost name from email address\"\"\"\n",
    "        logging.info(f\"Attempting to extract listhost name from: {email}\")\n",
    "        \n",
    "        if not email:\n",
    "            logging.warning(\"Empty email provided\")\n",
    "            return None\n",
    "            \n",
    "        if '@lists.uchicago.edu' not in email:\n",
    "            logging.warning(f\"Invalid listhost email format: {email}\")\n",
    "            return None\n",
    "            \n",
    "        listhost_name = email.split('@')[0].strip()\n",
    "        logging.info(f\"Extracted listhost name: {listhost_name}\")\n",
    "        return listhost_name\n",
    "        \n",
    "    def is_already_subscribed(self, listhost_name: str) -> bool:\n",
    "        \"\"\"Check if already subscribed to listhost\"\"\"\n",
    "        try:\n",
    "            # Navigate to info page\n",
    "            self.driver.get(f\"https://lists.uchicago.edu/web/info/{listhost_name}\")\n",
    "            time.sleep(5)  # Wait for page load\n",
    "            \n",
    "            # Look for unsubscribe link\n",
    "            unsubscribe_links = self.driver.find_elements(\n",
    "                By.XPATH, \n",
    "                f\"//a[@href='/web/signoff/{listhost_name}?previous_action=info']\"\n",
    "            )\n",
    "            return len(unsubscribe_links) > 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking subscription status for {listhost_name}: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def subscribe_to_listhost(self, listhost_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Attempt to subscribe to a single listhost\n",
    "        Returns True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if listhost_name in self.subscription_attempts:\n",
    "            logging.info(f\"Already attempted {listhost_name}, skipping\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Check if already subscribed\n",
    "            if self.is_already_subscribed(listhost_name):\n",
    "                logging.info(f\"Already subscribed to {listhost_name}\")\n",
    "                self.subscription_attempts[listhost_name] = {\n",
    "                    'timestamp': time.time(),\n",
    "                    'status': 'already_subscribed'\n",
    "                }\n",
    "                return False\n",
    "                \n",
    "            # Click subscribe link\n",
    "            subscribe_link = self.wait.until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.XPATH, f\"//a[@href='/web/subscribe/{listhost_name}?previous_action=info']\")\n",
    "                )\n",
    "            )\n",
    "            subscribe_link.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Click \"I subscribe\" button\n",
    "            subscribe_button = self.wait.until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.XPATH, f\"//input[@value='I subscribe to list {listhost_name}']\")\n",
    "                )\n",
    "            )\n",
    "            subscribe_button.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Click confirm button\n",
    "            confirm_button = self.wait.until(\n",
    "                EC.element_to_be_clickable(\n",
    "                    (By.ID, \"response_action_confirm\")\n",
    "                )\n",
    "            )\n",
    "            confirm_button.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Verify subscription was successful\n",
    "            success = self.is_already_subscribed(listhost_name)\n",
    "            \n",
    "            self.subscription_attempts[listhost_name] = {\n",
    "                'timestamp': time.time(),\n",
    "                'status': 'success' if success else 'failed'\n",
    "            }\n",
    "            \n",
    "            self.save_subscription_log()\n",
    "            return success\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error subscribing to {listhost_name}: {str(e)}\")\n",
    "            self.subscription_attempts[listhost_name] = {\n",
    "                'timestamp': time.time(),\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.save_subscription_log()\n",
    "            return False\n",
    "            \n",
    "    def process_rso_batch(self, rso_data_path: str):\n",
    "        \"\"\"\n",
    "        Process a batch of RSO subscriptions up to max_subscriptions\n",
    "        \"\"\"\n",
    "        # Load RSO data\n",
    "        with open(rso_data_path, 'r') as f:\n",
    "            rsos = json.load(f)\n",
    "            \n",
    "        subscriptions_processed = 0\n",
    "        \n",
    "        for rso in rsos:\n",
    "            if subscriptions_processed >= self.max_subscriptions:\n",
    "                logging.info(\"Reached maximum subscriptions for this batch\")\n",
    "                break\n",
    "                \n",
    "            listhost_email = rso.get('additional_info', {}).get('RSO Listhost')\n",
    "            if not listhost_email:\n",
    "                continue\n",
    "                \n",
    "            listhost_name = self.extract_listhost_name(listhost_email)\n",
    "            if not listhost_name:\n",
    "                continue\n",
    "                \n",
    "            if self.subscribe_to_listhost(listhost_name):\n",
    "                subscriptions_processed += 1\n",
    "                \n",
    "        logging.info(f\"Batch complete. Processed {subscriptions_processed} subscriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_subscription_batch(email: str, password: str, rso_data_path: str):\n",
    "    logging.info(\"Starting subscription batch process\")\n",
    "    \n",
    "    subscriber = ListhostSubscriber(email, password)\n",
    "    try:\n",
    "        logging.info(\"Attempting to login...\")\n",
    "        if subscriber.login():\n",
    "            logging.info(\"Login successful! Loading RSO data...\")\n",
    "            \n",
    "            # Load and categorize RSO data\n",
    "            subscriber.load_rso_data(rso_data_path)\n",
    "            \n",
    "            # Print initial status\n",
    "            status = subscriber.get_subscription_status()\n",
    "            logging.info(f\"Initial status: {status}\")\n",
    "            \n",
    "            # Process batch of pending subscriptions\n",
    "            logging.info(\"Starting to process pending subscriptions...\")\n",
    "            subscriber.process_pending_batch()\n",
    "            \n",
    "            # Print final status\n",
    "            status = subscriber.get_subscription_status()\n",
    "            logging.info(f\"Final status: {status}\")\n",
    "        else:\n",
    "            logging.error(\"Failed to login\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during subscription process: {str(e)}\")\n",
    "    finally:\n",
    "        logging.info(\"Closing browser...\")\n",
    "        subscriber.close()\n",
    "        logging.info(\"Process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting script\")\n",
    "\n",
    "try:\n",
    "    with open('../login.json', 'r') as file:\n",
    "        login = json.load(file)\n",
    "        logging.info(\"Successfully loaded login credentials\")\n",
    "        \n",
    "    run_subscription_batch(\n",
    "        login['email'],\n",
    "        login['password'],\n",
    "        '../rso_data_detailed.json'\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Script failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: listhost/scrape_email.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to scrape emails from given gmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "import base64\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmail_service():\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=8080)\n",
    "        \n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return build('gmail', 'v1', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_body(payload):\n",
    "    if 'parts' in payload:\n",
    "        for part in payload['parts']:\n",
    "            if part['mimeType'] == 'text/plain':\n",
    "                try:\n",
    "                    return base64.urlsafe_b64decode(part['body']['data']).decode()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding email body: {e}\")\n",
    "                    return None\n",
    "    elif payload.get('body', {}).get('data'):\n",
    "        try:\n",
    "            return base64.urlsafe_b64decode(payload['body']['data']).decode()\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding email body: {e}\")\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_emails(service):\n",
    "    try:\n",
    "        # Get all messages in inbox\n",
    "        results = service.users().messages().list(userId='me').execute()\n",
    "        messages = results.get('messages', [])\n",
    "        \n",
    "        if not messages:\n",
    "            print(\"No emails found.\")\n",
    "            return []\n",
    "        \n",
    "        email_list = []\n",
    "        print(f\"Found {len(messages)} emails. Processing...\")\n",
    "        \n",
    "        for message in messages:\n",
    "            msg = service.users().messages().get(userId='me', id=message['id']).execute()\n",
    "            \n",
    "            # Get subject from headers\n",
    "            subject = ''\n",
    "            sender = ''\n",
    "            for header in msg['payload']['headers']:\n",
    "                if header['name'] == 'Subject':\n",
    "                    subject = header['value']\n",
    "                elif header['name'] == 'From':\n",
    "                    sender = header['value']\n",
    "            \n",
    "            # Get body\n",
    "            body = get_email_body(msg['payload'])\n",
    "            \n",
    "            email_data = {\n",
    "                'subject': subject,\n",
    "                'from': sender,\n",
    "                'body': body\n",
    "            }\n",
    "            email_list.append(email_data)\n",
    "            \n",
    "            print(f\"\\nEmail:\")\n",
    "            print(f\"From: {sender}\")\n",
    "            print(f\"Subject: {subject}\")\n",
    "            print(f\"Body: {body[:200]}...\" if body else \"No plain text body found\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return email_list\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = get_gmail_service()\n",
    "emails = fetch_all_emails(service)\n",
    "print(f\"\\nTotal emails processed: {len(emails)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: scrape_db/blueprint_db_pipeline.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "INDEX_NAME = \"rso-chatbot\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def safe_get(dictionary: Dict, key: str, default: Any = \"None\") -> Any:\n",
    "    \"\"\"Safely get a value from a dictionary, returning default if None or missing.\"\"\"\n",
    "    value = dictionary.get(key)\n",
    "    return default if value is None else value\n",
    "\n",
    "def generate_safe_id(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a safe ASCII ID from a string.\n",
    "    Handles unicode characters, spaces, and special characters.\n",
    "    \"\"\"\n",
    "    if not name or name.lower() == \"none\":\n",
    "        return f\"unknown-rso-{hash(str(name))}\"\n",
    "        \n",
    "    # Convert to ASCII, remove diacritics\n",
    "    normalized = unicodedata.normalize('NFKD', name)\n",
    "    ascii_name = normalized.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Replace special characters and spaces with hyphens\n",
    "    safe_id = re.sub(r'[^a-zA-Z0-9]+', '-', ascii_name.lower())\n",
    "    \n",
    "    # Remove leading/trailing hyphens\n",
    "    safe_id = safe_id.strip('-')\n",
    "    \n",
    "    # Ensure we have a valid ID\n",
    "    if not safe_id:\n",
    "        return f\"unnamed-rso-{hash(name)}\"\n",
    "        \n",
    "    return safe_id\n",
    "\n",
    "def transform_rso_data(rso_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Transform RSO data for Pinecone database with safe None handling.\"\"\"\n",
    "    # Handle None case for entire RSO\n",
    "    if rso_data is None:\n",
    "        return {\n",
    "            \"name\": \"None\",\n",
    "            \"full_url\": \"None\",\n",
    "            \"description\": \"None\",\n",
    "            \"categories\": [],\n",
    "            \"contact_email\": \"None\",\n",
    "            \"additional_info\": {},\n",
    "            \"social_media_links\": []\n",
    "        }\n",
    "    \n",
    "    # Extract high confidence AI categories with None handling\n",
    "    ai_categories = safe_get(rso_data, \"ai_categories\", [])\n",
    "    high_confidence_categories = [\n",
    "        cat[\"name\"] \n",
    "        for cat in ai_categories \n",
    "        if cat.get(\"confidence\", 0) >= 85 and cat.get(\"name\")\n",
    "    ]\n",
    "    \n",
    "    # Combine with original categories, remove duplicates, handle None\n",
    "    original_categories = safe_get(rso_data, \"categories\", [])\n",
    "    all_categories = list(set(\n",
    "        [cat for cat in (original_categories + high_confidence_categories) if cat]\n",
    "    ))\n",
    "    \n",
    "    # Use full_description if available, otherwise fall back to description_preview\n",
    "    description = (\n",
    "        safe_get(rso_data, \"full_description\") \n",
    "        if rso_data.get(\"full_description\")\n",
    "        else safe_get(rso_data, \"description_preview\")\n",
    "    )\n",
    "    \n",
    "    # Safely get contact information\n",
    "    contact = safe_get(rso_data, \"contact\", {})\n",
    "    contact_email = safe_get(contact, \"email\") if isinstance(contact, dict) else \"None\"\n",
    "    \n",
    "    # Safely get and flatten additional_info\n",
    "    additional_info = safe_get(rso_data, \"additional_info\", {})\n",
    "    flattened_additional_info = {\n",
    "        str(k): str(v) if v is not None else \"None\"\n",
    "        for k, v in additional_info.items()\n",
    "    } if isinstance(additional_info, dict) else {}\n",
    "    \n",
    "    # Safely get social media links\n",
    "    social_media = safe_get(rso_data, \"social_media\", {})\n",
    "    social_media_links = [\n",
    "        str(link) for link in social_media.values()\n",
    "        if link is not None\n",
    "    ] if isinstance(social_media, dict) else []\n",
    "    \n",
    "    # Create transformed dictionary with only desired fields and proper types\n",
    "    transformed_data = {\n",
    "        \"name\": safe_get(rso_data, \"name\"),\n",
    "        \"full_url\": safe_get(rso_data, \"full_url\"),\n",
    "        \"description\": description,\n",
    "        \"categories\": all_categories,\n",
    "        \"contact_email\": contact_email,\n",
    "        \"additional_info\": flattened_additional_info,\n",
    "        \"social_media_links\": social_media_links\n",
    "    }\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def generate_embedding(rso_data: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Generate embedding for RSO data.\"\"\"\n",
    "    # Combine relevant text fields for embedding\n",
    "    text_to_embed = f\"{rso_data['name']} {rso_data['description']} {' '.join(rso_data['categories'])}\"\n",
    "    \n",
    "    # Generate embedding\n",
    "    embedding = model.encode(text_to_embed)\n",
    "    return embedding\n",
    "\n",
    "def prepare_pinecone_data(rso_data: Dict[str, Any], embedding: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Prepare data for Pinecone upsert.\"\"\"\n",
    "    # Generate safe ASCII ID\n",
    "    safe_id = generate_safe_id(rso_data[\"name\"])\n",
    "    \n",
    "    return {\n",
    "        \"id\": safe_id,\n",
    "        \"values\": embedding.tolist(),\n",
    "        \"metadata\": {\n",
    "            \"name\": rso_data[\"name\"],\n",
    "            \"full_url\": rso_data[\"full_url\"],\n",
    "            \"description\": rso_data[\"description\"],\n",
    "            \"categories\": rso_data[\"categories\"],\n",
    "            \"contact_email\": rso_data[\"contact_email\"],\n",
    "            \"social_media_links\": rso_data[\"social_media_links\"],\n",
    "            \"additional_info\": [f\"{k}: {v}\" for k, v in rso_data[\"additional_info\"].items()]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_all_rsos(data, batch_size=100):\n",
    "    \"\"\"Process all RSOs and upsert to Pinecone in batches.\"\"\"\n",
    "    # Ensure data is a list\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    vectors = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, rso in enumerate(data):\n",
    "        try:\n",
    "            transformed_data = transform_rso_data(rso)\n",
    "            embedding = generate_embedding(transformed_data)\n",
    "            vector = prepare_pinecone_data(transformed_data, embedding)\n",
    "            vectors.append(vector)\n",
    "            successful += 1\n",
    "            \n",
    "            # Upsert when batch is full\n",
    "            if len(vectors) >= batch_size:\n",
    "                index.upsert(vectors=vectors)\n",
    "                print(f\"Upserted batch of {len(vectors)} vectors\")\n",
    "                vectors = []\n",
    "                \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} RSOs (Successful: {successful}, Failed: {failed})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"Error processing RSO {rso.get('name', 'unknown')}: {str(e)}\")\n",
    "    \n",
    "    # Upsert any remaining vectors\n",
    "    if vectors:\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Upserted final batch of {len(vectors)} vectors\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Total RSOs processed: {len(data)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_rsos(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: scrape_db/categorize_rso.ipynb
================
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to add better categories to RSO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from groq import Groq\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import tiktoken\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # This loads the variables from .env\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    ")\n",
    "token_bucket = TokenBucket(tokens_per_minute=4500)  # Using 4500 to be conservative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenBucket:\n",
    "    def __init__(self, tokens_per_minute: int = 5000):\n",
    "        self.max_tokens = tokens_per_minute\n",
    "        self.tokens = tokens_per_minute\n",
    "        self.last_update = datetime.now()\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "\n",
    "    def update_tokens(self):\n",
    "        now = datetime.now()\n",
    "        time_passed = now - self.last_update\n",
    "        self.tokens = min(\n",
    "            self.max_tokens,\n",
    "            self.tokens + (time_passed.total_seconds() * self.tokens_per_minute / 60)\n",
    "        )\n",
    "        self.last_update = now\n",
    "\n",
    "    def consume(self, tokens: int) -> float:\n",
    "        self.update_tokens()\n",
    "        if self.tokens < tokens:\n",
    "            wait_time = (tokens - self.tokens) * 60 / self.tokens_per_minute\n",
    "            return wait_time\n",
    "        self.tokens -= tokens\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Estimate token count using tiktoken\"\"\"\n",
    "    # Using cl100k_base as an approximation\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def exponential_backoff(attempt: int, base_delay: float = 1) -> float:\n",
    "    \"\"\"Calculate delay with jitter for exponential backoff\"\"\"\n",
    "    delay = min(300, base_delay * (2 ** attempt))  # Cap at 5 minutes\n",
    "    jitter = random.uniform(0, 0.1 * delay)\n",
    "    return delay + jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CATEGORIES = [\n",
    "    \"Biology\", \"Business\", \"Chemistry\", \"Physics\", \"Mathematics\", \"Computer Science\", \n",
    "    \"Data Science\", \"Economics\", \"Psychology\", \"Sociology\", \"Political Science\",\n",
    "    \"History\", \"Philosophy\", \"Literature\", \"Languages\", \"Law\", \"Medicine\",\n",
    "    \"Nursing\", \"Public Health\", \"Engineering\", \"Environmental Science\",\n",
    "    \"Finance\", \"Investment\", \"Quantitative Trading\", \"Private Equity\",\n",
    "    \"Venture Capital\", \"Consulting\", \"Marketing\", \"Entrepreneurship\",\n",
    "    \"Real Estate\", \"Technology\", \"Software Development\", \"Product Management\",\n",
    "    \"Healthcare\", \"Legal\", \"Research\", \"Journalism\", \"Media Production\",\n",
    "    \"Visual Arts\", \"Painting\", \"Photography\", \"Digital Art\", \"Music\",\n",
    "    \"Band\", \"Choir\", \"A Cappella\", \"Theater\", \"Dance\", \"Film\",\n",
    "    \"Creative Writing\", \"Design\", \"Cultural\", \"International\", \"Religious\",\n",
    "    \"LGBTQ+\", \"Gender & Sexuality\", \"Social Justice\", \"Political\", \"Activism\",\n",
    "    \"Community Service/volunteering\", \"Mentorship\", \"Environmental\", \"Sports\"\n",
    "    \"Team Sports\", \"Individual Sports\", \"Gaming\",\n",
    "    \"Debate\", \"Model UN\", \"Food & Cooking\", \"Travel\", \"Outdoor Activities\",\n",
    "    \"Student Government\", \"Publications\", \"Journalism\", \"Mental Health\", \"Wellness\",\n",
    "    \"Career Development\", \"Academic Support\", \"Leadership\", \"Greek Life\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"name\": \"Investment Banking Group\",\n",
    "        \"description\": \"A professional organization dedicated to educating members about investment banking, private equity, and financial markets. We host networking events, technical workshops, and mock interviews to prepare students for careers in finance.\",\n",
    "        \"ideal_categories\": [\"Finance\", \"Investment\", \"Career Development\"],\n",
    "        \"explanation\": \"This RSO focuses on finance education and career preparation, warranting multiple related financial categories.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Data Science for Social Good\",\n",
    "        \"description\": \"We apply data science and machine learning techniques to tackle social issues in healthcare, education, and environmental sustainability. Members work on real-world projects while learning technical skills.\",\n",
    "        \"ideal_categories\": [\"Data Science\", \"Computer Science\", \"Community Service/volunteering\"],\n",
    "        \"explanation\": \"Combines technical data science work with social impact, deserving both technical and service categories.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Mental Health Alliance\",\n",
    "        \"description\": \"A student organization focused on promoting mental health awareness, reducing stigma, and connecting students with resources. We organize wellness workshops, peer support groups, and educational events.\",\n",
    "        \"ideal_categories\": [\"Mental Health\", \"Wellness\", \"Student Life\"],\n",
    "        \"explanation\": \"Focuses on mental health and wellness within student life context.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(rso: Dict) -> str:\n",
    "    \"\"\"Create a prompt for the LLM to categorize an RSO.\"\"\"\n",
    "    few_shot_text = \"\\n---\\n\".join([\n",
    "        f\"\"\"\n",
    "Name: {ex['name']}\n",
    "Description: {ex['description']}\n",
    "Categories: {', '.join(ex['ideal_categories'])}\n",
    "Explanation: {ex['explanation']}\"\"\" \n",
    "        for ex in FEW_SHOT_EXAMPLES\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert at categorizing university student organizations. Given an RSO's name and description, assign it relevant categories from the provided list. Each RSO can have multiple categories if appropriate.\n",
    "\n",
    "Valid categories: {', '.join(VALID_CATEGORIES)}\n",
    "\n",
    "Here are some examples:\n",
    "{few_shot_text}\n",
    "\n",
    "For the following RSO, please provide:\n",
    "1. A list of relevant categories (can be multiple)\n",
    "2. A confidence score (0-100) for each category\n",
    "3. A brief explanation of your categorization\n",
    "\n",
    "Name: {rso['name']}\n",
    "Description: {rso.get('full_description', '') or rso.get('description_preview', '')}\n",
    "\n",
    "Response should be in JSON format:\n",
    "{{\n",
    "  \"categories\": [\n",
    "    {{\"name\": \"category_name\", \"confidence\": 95}},\n",
    "    {{\"name\": \"another_category\", \"confidence\": 85}}\n",
    "  ],\n",
    "  \"explanation\": \"Brief explanation of categorization\"\n",
    "}}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_rso(rso: Dict, attempt: int = 0) -> Optional[Dict]:\n",
    "    \"\"\"Categorize a single RSO using the Groq API with rate limiting.\"\"\"\n",
    "    try:\n",
    "        prompt = create_prompt(rso)\n",
    "        estimated_tokens = count_tokens(prompt) + 500  # Add buffer for response\n",
    "        \n",
    "        # Check token bucket\n",
    "        wait_time = token_bucket.consume(estimated_tokens)\n",
    "        if wait_time > 0:\n",
    "            print(f\"\\nRate limit approaching, waiting {wait_time:.2f} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        response = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        # Validate categories against our list\n",
    "        response['categories'] = [\n",
    "            cat for cat in response['categories'] \n",
    "            if cat['name'] in VALID_CATEGORIES\n",
    "        ]\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        if \"rate_limit\" in str(e).lower():\n",
    "            if attempt < 5:  # Max 5 retries\n",
    "                delay = exponential_backoff(attempt)\n",
    "                print(f\"\\nRate limit hit for {rso['name']}, waiting {delay:.2f} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                return categorize_rso(rso, attempt + 1)\n",
    "            else:\n",
    "                print(f\"\\nMax retries reached for {rso['name']}\")\n",
    "        print(f\"Error categorizing RSO {rso['name']}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rsos(input_file: str = None, output_file: str = None, rsos: List[Dict] = None):\n",
    "    \"\"\"Process all RSOs from input file or list and save results to output file.\"\"\"\n",
    "    try:\n",
    "        # Read input JSON if file provided, otherwise use provided list\n",
    "        if input_file:\n",
    "            with open(input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "        \n",
    "        if not rsos:\n",
    "            raise ValueError(\"No RSOs provided\")\n",
    "            \n",
    "        results = []\n",
    "        batch_size = 3  # Reduced batch size\n",
    "        \n",
    "        # Process RSOs in batches with progress bar\n",
    "        for i in tqdm(range(0, len(rsos), batch_size)):\n",
    "            batch = rsos[i:i + batch_size]\n",
    "            batch_results = []\n",
    "            \n",
    "            # Process each RSO in batch\n",
    "            for rso in batch:\n",
    "                categorization = categorize_rso(rso)\n",
    "                if categorization:\n",
    "                    rso['ai_categories'] = categorization['categories']\n",
    "                    rso['categorization_explanation'] = categorization['explanation']\n",
    "                batch_results.append(rso)\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # Save intermediate results every batch\n",
    "            if output_file:\n",
    "                with open(f\"{output_file}.partial\", 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "            \n",
    "            # Add delay between batches\n",
    "            time.sleep(2)  # Conservative delay between batches\n",
    "        \n",
    "        # Write final results to output file if provided\n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f'\\nCategorization complete! Results saved to {output_file}')\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'\\nError processing RSOs: {str(e)}')\n",
    "        # Save partial results if available\n",
    "        if results and output_file:\n",
    "            with open(f\"{output_file}.error_partial\", 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f'Partial results saved to {output_file}.error_partial')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_rsos('rso_data_detailed.json', 'categorized_rsos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: scrape_db/scrape_blueprint_all.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlueprintScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://blueprint.uchicago.edu/organizations\"\n",
    "        self.category_map = {\n",
    "            '4150': 'Academic Interest',\n",
    "            '4174': 'Campus and Student Life',\n",
    "            '4151': 'Community Service',\n",
    "            '4152': 'Cultural & Ethnic',\n",
    "            '4153': 'Fine Arts',\n",
    "            '8195': 'Graduate/Professional',\n",
    "            '4155': 'Media & Publication',\n",
    "            '4156': 'Political & Advocacy',\n",
    "            '4157': 'Religious & Spiritual',\n",
    "            '4158': 'Social',\n",
    "            '4159': 'Sports Clubs',\n",
    "            '4288': 'Student Government',\n",
    "            '4289': 'University Department/Program'\n",
    "        }\n",
    "        self.driver = None\n",
    "        self.rso_data = []\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver with appropriate options\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def load_all_rsos(self, expected_count=408):\n",
    "        \"\"\"Click 'Show More' button until all RSOs are loaded\n",
    "        \n",
    "        Args:\n",
    "            expected_count (int): Expected number of RSOs to load\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if expected number of RSOs were loaded, False otherwise\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting to load all RSOs (expecting {expected_count})...\")\n",
    "        page_source_length = 0\n",
    "        attempts = 0\n",
    "        max_attempts = 50  # Increased max attempts to ensure we get all RSOs\n",
    "        \n",
    "        while attempts < max_attempts:\n",
    "            # Check current number of RSOs\n",
    "            current_rsos = len(self.driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/organization/\"]'))\n",
    "            logger.info(f\"Currently loaded RSOs: {current_rsos}\")\n",
    "            \n",
    "            if current_rsos >= expected_count:\n",
    "                logger.info(f\"Successfully loaded all {current_rsos} RSOs\")\n",
    "                return True\n",
    "                \n",
    "            # Continue with show more clicks\n",
    "            try:\n",
    "                # Try to find and click the \"Show More\" button\n",
    "                show_more = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \n",
    "                    \"//button[.//span[contains(text(), 'Load More')]]\"))\n",
    "                )\n",
    "                \n",
    "                # If the button is not visible in viewport, scroll to it\n",
    "                self.driver.execute_script(\"\"\"\n",
    "                    var element = arguments[0];\n",
    "                    var headerOffset = 100;\n",
    "                    var elementPosition = element.getBoundingClientRect().top;\n",
    "                    var offsetPosition = elementPosition + window.pageYOffset - headerOffset;\n",
    "                    window.scrollTo({\n",
    "                        top: offsetPosition,\n",
    "                        behavior: 'smooth'\n",
    "                    });\n",
    "                \"\"\", show_more)\n",
    "                time.sleep(0.5)  # Short pause for scroll\n",
    "                \n",
    "                # Click the button\n",
    "                self.driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "                logger.info(f\"Clicked 'Load More' button, attempt {attempts + 1}\")\n",
    "                \n",
    "                # Wait for new content\n",
    "                time.sleep(2)  # Increased wait time for reliability\n",
    "                \n",
    "                # Check if page content has grown\n",
    "                new_length = len(self.driver.page_source)\n",
    "                if new_length == page_source_length:\n",
    "                    logger.info(\"No new content loaded, finishing...\")\n",
    "                    break\n",
    "                \n",
    "                page_source_length = new_length\n",
    "                attempts += 1\n",
    "                \n",
    "            except TimeoutException:\n",
    "                logger.info(\"No more 'Show More' button found, all content loaded\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error clicking 'Show More': {str(e)}\")\n",
    "                break\n",
    "                \n",
    "        # Final check after all attempts\n",
    "        final_count = len(self.driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/organization/\"]'))\n",
    "        logger.info(f\"Finished loading RSOs after {attempts} attempts. Final count: {final_count}\")\n",
    "        return final_count >= expected_count\n",
    "    \n",
    "    def extract_rso_info_from_card(self, card_element):\n",
    "        \"\"\"Extract RSO information from a card element\"\"\"\n",
    "        try:\n",
    "            # Get the link and RSO name from the href\n",
    "            link = card_element.get('href')\n",
    "            rso_name = link.split('/')[-1] if link else None\n",
    "            \n",
    "            # Get the display name (from img alt or other source)\n",
    "            img = card_element.find('img')\n",
    "            display_name = img.get('alt') if img else None\n",
    "            \n",
    "            # Get the description excerpt\n",
    "            description = card_element.find('p', class_='DescriptionExcerpt')\n",
    "            description_text = description.text.strip() if description else \"\"\n",
    "            \n",
    "            # Get the image URL\n",
    "            img_src = img.get('src') if img else None\n",
    "            \n",
    "            return {\n",
    "                'name': display_name,\n",
    "                'url_name': rso_name,\n",
    "                'full_url': urljoin(\"https://blueprint.uchicago.edu\", link) if link else None,\n",
    "                'description_preview': description_text,\n",
    "                'image_url': img_src\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting RSO card info: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_rso_cards(self):\n",
    "        \"\"\"Extract all RSO cards from the loaded page\"\"\"\n",
    "        logger.info(\"Starting to extract RSO cards...\")\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Find all RSO card links\n",
    "        rso_cards = soup.find_all('a', href=lambda x: x and x.startswith('/organization/'))\n",
    "        \n",
    "        rso_info = []\n",
    "        for card in rso_cards:\n",
    "            info = self.extract_rso_info_from_card(card)\n",
    "            if info:\n",
    "                rso_info.append(info)\n",
    "        \n",
    "        logger.info(f\"Found {len(rso_info)} RSO cards\")\n",
    "        return rso_info\n",
    "    \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Main function to scrape all RSOs\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting RSO scraping process...\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # First pass: Get all RSOs and their basic info\n",
    "            logger.info(\"Loading main page...\")\n",
    "            self.driver.get(self.base_url)\n",
    "            self.load_all_rsos()\n",
    "            rso_info = self.extract_rso_cards()\n",
    "            \n",
    "            if not rso_info:\n",
    "                logger.error(\"No RSO cards found!\")\n",
    "                return []\n",
    "            \n",
    "            if len(rso_info) < 408:\n",
    "                logger.error(f\"Only found {len(rso_info)} RSOs, expected 408. Retrying...\")\n",
    "                # Try one more time with longer waits\n",
    "                self.driver.get(self.base_url)\n",
    "                time.sleep(3)  # Longer initial wait\n",
    "                success = self.load_all_rsos(408)\n",
    "                if not success:\n",
    "                    logger.error(\"Failed to load all RSOs even after retry\")\n",
    "                rso_info = self.extract_rso_cards()\n",
    "                \n",
    "            if len(rso_info) < 408:\n",
    "                logger.error(f\"Warning: Only found {len(rso_info)} RSOs out of 408 expected\")\n",
    "            \n",
    "            # Save data to JSON file\n",
    "            with open('rso_data.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(rso_info, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "            logger.info(f\"Successfully scraped {len(rso_info)} RSOs\")\n",
    "            return rso_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = BlueprintScraper()\n",
    "scraper.scrape_all_rsos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_field_from_json(file_path, field_to_remove):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # If the data is a dictionary, remove the field\n",
    "    if isinstance(data, dict):\n",
    "        if field_to_remove in data:\n",
    "            del data[field_to_remove]\n",
    "    # If the data is a list of dictionaries, remove the field from each item\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            if isinstance(item, dict) and field_to_remove in item:\n",
    "                del item[field_to_remove]\n",
    "    \n",
    "    # Write the updated data back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "file_path = 'rso_data.json'\n",
    "field_to_remove = 'image_url'\n",
    "\n",
    "try:\n",
    "    remove_field_from_json(file_path, field_to_remove)\n",
    "    print(f\"Successfully removed '{field_to_remove}' from {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CategoryCollector:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://blueprint.uchicago.edu/organizations\"\n",
    "        self.category_map = {\n",
    "            '4150': 'Academic Interest',\n",
    "            '4174': 'Campus and Student Life',\n",
    "            '4151': 'Community Service',\n",
    "            '4152': 'Cultural & Ethnic',\n",
    "            '4153': 'Fine Arts',\n",
    "            '8195': 'Graduate/Professional',\n",
    "            '4155': 'Media & Publication',\n",
    "            '4156': 'Political & Advocacy',\n",
    "            '4157': 'Religious & Spiritual',\n",
    "            '4158': 'Social',\n",
    "            '4159': 'Sports Clubs',\n",
    "            '4288': 'Student Government',\n",
    "            '4289': 'University Department/Program'\n",
    "        }\n",
    "        self.driver = None\n",
    "        self.rso_categories = defaultdict(set)\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    def load_all_rsos(self, category_name=\"\"):\n",
    "        logger.info(f\"Loading RSOs for {category_name}...\")\n",
    "        attempts = 0\n",
    "        max_attempts = 50\n",
    "        \n",
    "        while attempts < max_attempts:\n",
    "            try:\n",
    "                show_more = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \n",
    "                    \"//button[.//span[contains(text(), 'Load More')]]\"))\n",
    "                )\n",
    "                \n",
    "                self.driver.execute_script(\"\"\"\n",
    "                    var element = arguments[0];\n",
    "                    var headerOffset = 100;\n",
    "                    var elementPosition = element.getBoundingClientRect().top;\n",
    "                    var offsetPosition = elementPosition + window.pageYOffset - headerOffset;\n",
    "                    window.scrollTo({\n",
    "                        top: offsetPosition,\n",
    "                        behavior: 'smooth'\n",
    "                    });\n",
    "                \"\"\", show_more)\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                self.driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "                time.sleep(2)\n",
    "                attempts += 1\n",
    "                \n",
    "            except TimeoutException:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in load_all_rsos for {category_name}: {str(e)}\")\n",
    "                break\n",
    "    \n",
    "    def collect_categories(self):\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Load existing RSO data\n",
    "            with open('rso_data.json', 'r', encoding='utf-8') as f:\n",
    "                rso_data = json.load(f)\n",
    "            \n",
    "            # Create lookup of url_names\n",
    "            all_rsos = {rso['url_name'] for rso in rso_data if rso.get('url_name')}\n",
    "            logger.info(f\"Loaded {len(all_rsos)} RSOs from existing data\")\n",
    "            \n",
    "            # Check each category\n",
    "            for cat_id, cat_name in self.category_map.items():\n",
    "                try:\n",
    "                    logger.info(f\"Checking category: {cat_name}\")\n",
    "                    self.driver.get(f\"{self.base_url}?categories={cat_id}\")\n",
    "                    self.load_all_rsos(cat_name)\n",
    "                    \n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    category_links = soup.find_all('a', href=lambda x: x and x.startswith('/organization/'))\n",
    "                    category_rsos = {link['href'].split('/')[-1] for link in category_links}\n",
    "                    \n",
    "                    # Add category to each RSO found\n",
    "                    for rso in category_rsos:\n",
    "                        self.rso_categories[rso].add(cat_name)\n",
    "                    \n",
    "                    logger.info(f\"Found {len(category_rsos)} RSOs in {cat_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing category {cat_name}: {str(e)}\")\n",
    "            \n",
    "            # Update the existing data with categories\n",
    "            for rso in rso_data:\n",
    "                url_name = rso.get('url_name')\n",
    "                if url_name:\n",
    "                    rso['categories'] = list(self.rso_categories.get(url_name, set()))\n",
    "            \n",
    "            # Save updated data\n",
    "            with open('rso_data_with_categories.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(rso_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Log category statistics\n",
    "            logger.info(\"\\nCategory Statistics:\")\n",
    "            for cat_name in self.category_map.values():\n",
    "                count = sum(1 for cats in self.rso_categories.values() if cat_name in cats)\n",
    "                logger.info(f\"  {cat_name}: {count} RSOs\")\n",
    "            \n",
    "            no_category_count = sum(1 for rso in all_rsos if not self.rso_categories.get(rso))\n",
    "            logger.info(f\"RSOs with no category: {no_category_count}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in category collection: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collector = CategoryCollector()\n",
    "collector.collect_categories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: scrape_db/scrape_blueprint_ind.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json', \n",
    "                 checkpoint_file='rso_scraping_checkpoint.json',\n",
    "                 checkpoint_frequency=10):  # Save every 10 RSOs\n",
    "        self.input_file = input_file\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        self.driver = None\n",
    "        \n",
    "    def save_checkpoint(self, rsos, last_processed_index):\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'last_processed_index': last_processed_index,\n",
    "            'rsos': rsos\n",
    "        }\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        logger.info(f\"Checkpoint saved at index {last_processed_index}\")\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load progress from checkpoint file if it exists\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                checkpoint_data = json.load(f)\n",
    "            logger.info(f\"Resuming from checkpoint at index {checkpoint_data['last_processed_index']}\")\n",
    "            return checkpoint_data['last_processed_index'], checkpoint_data['rsos']\n",
    "        except FileNotFoundError:\n",
    "            logger.info(\"No checkpoint found, starting from beginning\")\n",
    "            return -1, None\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing extra whitespace and asterisks\"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        return text.strip('* ').strip()\n",
    "    \n",
    "    def scrape_detail_page(self, url):\n",
    "        \"\"\"Scrape a single RSO detail page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(1)  # Allow page to load\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            details = {\n",
    "                'full_description': '',\n",
    "                'contact': {},\n",
    "                'additional_info': {}\n",
    "            }\n",
    "            \n",
    "            # Get full description from the correct div class and all paragraph tags within\n",
    "            description_div = soup.find('div', class_='bodyText-large userSupplied')\n",
    "            if description_div:\n",
    "                paragraphs = description_div.find_all('p')\n",
    "                details['full_description'] = ' '.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            \n",
    "            # Get contact information - updated for new structure\n",
    "            contact_email_div = soup.find('span', class_='sr-only', string='Contact Email')\n",
    "            if contact_email_div and contact_email_div.parent:\n",
    "                email_text = contact_email_div.parent.get_text()\n",
    "                # Extract the email from the text (everything after \"E: \")\n",
    "                if 'E:' in email_text:\n",
    "                    email = email_text.split('E:')[1].strip().strip('\"')\n",
    "                    details['contact']['email'] = email\n",
    "                \n",
    "            # Get address if available\n",
    "            address_div = soup.find('div', string='Address')\n",
    "            if address_div and address_div.find_next('div'):\n",
    "                details['contact']['address'] = self.clean_text(address_div.find_next('div').get_text())\n",
    "            \n",
    "            \n",
    "            # Get social media links and website\n",
    "            social_media = {}\n",
    "            \n",
    "            # Get website from aria-label\n",
    "            website_link = soup.find('a', attrs={'aria-label': lambda x: x and 'Visit our site' in x})\n",
    "            if website_link and website_link.get('href'):\n",
    "                social_media['website'] = website_link['href']\n",
    "            \n",
    "            # Get social media links\n",
    "            social_links = soup.find_all('a', href=True)\n",
    "            for link in social_links:\n",
    "                href = link['href']\n",
    "                if 'facebook.com' in href:\n",
    "                    social_media['facebook'] = href\n",
    "                elif 'instagram.com' in href:\n",
    "                    social_media['instagram'] = href\n",
    "            \n",
    "            if social_media:\n",
    "                details['social_media'] = social_media\n",
    "\n",
    "\n",
    "           # Get additional information\n",
    "            additional_info_h2 = soup.find('h2', string=lambda x: x and 'Additional Information' in x)\n",
    "            if additional_info_h2:\n",
    "                # Get to the container div\n",
    "                container = additional_info_h2.parent.parent.find_next_sibling('div')\n",
    "                if container:\n",
    "                    # Find all field divs by their specific style\n",
    "                    field_divs = container.find_all('div', style=lambda x: x and 'padding-bottom: 8px; margin-left: 15px;' in x)\n",
    "                    \n",
    "                    for field_div in field_divs:\n",
    "                        # Get the label from the strong tag\n",
    "                        label_div = field_div.find('div', style='font-weight: bold;')\n",
    "                        if label_div and label_div.strong:\n",
    "                            label = label_div.strong.text.strip()\n",
    "                            \n",
    "                            # Get the value by getting the second div (skipping the label div)\n",
    "                            divs = field_div.find_all('div', recursive=False)\n",
    "                            if len(divs) >= 2:  # Make sure we have both label and value divs\n",
    "                                value_div = divs[1].find('div')  # Get the inner div of the second div\n",
    "                                if value_div:\n",
    "                                    value = value_div.text.strip()\n",
    "                                    details['additional_info'][label] = value\n",
    "                    \n",
    "                    logger.info(f\"Extracted additional info: {details['additional_info']}\")\n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "            \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs with checkpointing\"\"\"\n",
    "        try:\n",
    "            # Try to load from checkpoint first\n",
    "            last_processed_index, checkpoint_rsos = self.load_checkpoint()\n",
    "            \n",
    "            # If no checkpoint, load from input file\n",
    "            if checkpoint_rsos is None:\n",
    "                with open(self.input_file, 'r') as f:\n",
    "                    rsos = json.load(f)\n",
    "                start_index = 0\n",
    "            else:\n",
    "                rsos = checkpoint_rsos\n",
    "                start_index = last_processed_index + 1\n",
    "            \n",
    "            logger.info(f\"Processing {len(rsos)} RSOs starting from index {start_index}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i in range(start_index, len(rsos)):\n",
    "                rso = rsos[i]\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if (i + 1) % self.checkpoint_frequency == 0:\n",
    "                    self.save_checkpoint(rsos, i)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save final results\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            # Clean up checkpoint file after successful completion\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "                \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "            # Save checkpoint on error\n",
    "            if 'rsos' in locals() and 'i' in locals():\n",
    "                self.save_checkpoint(rsos, i)\n",
    "                logger.info(\"Progress saved to checkpoint file after error\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json'):\n",
    "        self.input_file = input_file\n",
    "        self.driver = None\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(rsos)} RSOs from {self.input_file}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i, rso in enumerate(rsos):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save updated data to new file\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "    \n",
    "\n",
    "\n",
    "    # test method\n",
    "    def test_detail_scraping(self, num_test_pages=3):\n",
    "        \"\"\"Test the detail scraping on a few RSO pages and print results\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Running test on {num_test_pages} RSO pages...\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            for i, rso in enumerate(rsos[:num_test_pages]):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"\\nTesting RSO {i+1}: {rso.get('name', 'Unknown')}\")\n",
    "                logger.info(f\"URL: {url}\")\n",
    "                \n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                # Print detailed results\n",
    "                if details:\n",
    "                    logger.info(\"\\nScraped Details:\")\n",
    "                    logger.info(f\"Description: {details['full_description'][:200]}...\")\n",
    "                    logger.info(f\"Contact Info: {details['contact']}\")\n",
    "                    logger.info(f\"Social Media: {details.get('social_media', {})}\")\n",
    "                    logger.info(f\"Additional Info: {details['additional_info']}\")\n",
    "                else:\n",
    "                    logger.error(\"Failed to scrape details\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in test process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RSODetailScraper()\n",
    "scraper.scrape_all_rsos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RSODetailScraper()\n",
    "# Run test first\n",
    "scraper.test_detail_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: .gitignore
================
credentials.json
login.json
token.pickle
listhost_login.log
.env
*json

================
File: chatbot1.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSORagBot:\n",
    "    def __init__(self, pinecone_api_key: str, pinecone_index_name: str, groq_api_key: str):\n",
    "        # Initialize Pinecone\n",
    "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
    "        self.index = self.pc.Index(pinecone_index_name)\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embed_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        \n",
    "        # Initialize Groq client\n",
    "        self.groq_client = Groq(api_key=groq_api_key)\n",
    "        \n",
    "        # Define system prompt\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant for University of Chicago students, \n",
    "        helping them find Registered Student Organizations (RSOs) that match their interests. \n",
    "        Use the provided RSO information to give detailed, relevant recommendations. \n",
    "        Always include the RSO's name, a brief description, and contact information when available.\"\"\"\n",
    "\n",
    "    def get_relevant_rsos(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get relevant RSOs based on the query\n",
    "        \"\"\"\n",
    "        # Create query embedding\n",
    "        query_embedding = self.embed_model.encode(query).tolist()\n",
    "        \n",
    "        # Query Pinecone\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        return results.matches\n",
    "\n",
    "    def format_context(self, relevant_rsos: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Format RSO information into a context string for the LLM\n",
    "        \"\"\"\n",
    "        context = \"Here are some relevant RSOs:\\n\\n\"\n",
    "        \n",
    "        for rso in relevant_rsos:\n",
    "            metadata = rso.metadata\n",
    "            context += f\"Name: {metadata['name']}\\n\"\n",
    "            context += f\"Description: {metadata['description']}\\n\"\n",
    "            context += f\"Categories: {', '.join(metadata['categories'])}\\n\"\n",
    "            context += f\"Contact: {metadata['contact_email']}\\n\"\n",
    "            if metadata['social_media_links']:\n",
    "                context += f\"Social Media: {', '.join(metadata['social_media_links'])}\\n\"\n",
    "            if metadata['additional_info']:\n",
    "                context += f\"Additional Info: {', '.join(metadata['additional_info'])}\\n\"\n",
    "            context += f\"Website: {metadata['full_url']}\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def generate_response(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to the user's query\n",
    "        \"\"\"\n",
    "        # Get relevant RSOs\n",
    "        relevant_rsos = self.get_relevant_rsos(query)\n",
    "        \n",
    "        # Format context\n",
    "        context = self.format_context(relevant_rsos)\n",
    "        \n",
    "        # Construct the prompt\n",
    "        prompt = f\"\"\"Based on the following RSO information, please recommend appropriate RSOs for a student with this query: \"{query}\"\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Please provide a natural, conversational response that:\n",
    "        1. Highlights the most relevant RSOs for their interests\n",
    "        2. Explains why each RSO might be a good fit\n",
    "        3. Includes practical information like how to get involved\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get response from Groq\n",
    "        response = self.groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"mixtral-8x7b-32768\",  # or your preferred Groq model\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RSORagBot(\n",
    "        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "        pinecone_index_name=\"rso-chatbot\",  # replace with your index name\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "\n",
    "query = \"what's your rso question?\"\n",
    "response = bot.generate_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: README.md
================
# rsochatbot

Project to create chatbot capable of guiding students to RSOs and other organizations. 

TODO:

Scrape blueprint data 

Categorize data using LLMs? 

Create blueprint vector database 



Scrape listhost, create vector database that constantly updates
 - filter through listhosts to ignore irrelevant data, ex 2016 medschool alums



Idea: add ig if they have one?
