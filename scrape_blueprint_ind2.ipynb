{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from typing import List, Dict\n",
    "from aiohttp import ClientTimeout\n",
    "from asyncio import Semaphore\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncRSOScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json', concurrent_limit=3):\n",
    "        self.input_file = input_file\n",
    "        self.semaphore = Semaphore(concurrent_limit)\n",
    "        self.timeout = ClientTimeout(total=30)\n",
    "        \n",
    "    async def scrape_detail_page(self, session: aiohttp.ClientSession, url: str, rso: Dict) -> Dict:\n",
    "        async with self.semaphore:\n",
    "            try:\n",
    "                async with session.get(url, timeout=self.timeout) as response:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    details = {\n",
    "                        'full_description': '',\n",
    "                        'contact': {},\n",
    "                        'additional_info': {},\n",
    "                        'social_media': {}\n",
    "                    }\n",
    "                    \n",
    "                    # Description\n",
    "                    if desc_div := soup.find('div', class_='bodyText-large userSupplied'):\n",
    "                        details['full_description'] = ' '.join(p.get_text(strip=True) \n",
    "                            for p in desc_div.find_all('p') if p.get_text(strip=True))\n",
    "                    \n",
    "                    # Contact email\n",
    "                    if email_div := soup.find('span', class_='sr-only', string='Contact Email'):\n",
    "                        if email_text := email_div.parent.get_text():\n",
    "                            if 'E:' in email_text:\n",
    "                                details['contact']['email'] = email_text.split('E:')[1].strip().strip('\"')\n",
    "                    \n",
    "                    # Social media\n",
    "                    if website_link := soup.find('a', attrs={'aria-label': lambda x: x and 'Visit our site' in x}):\n",
    "                        details['social_media']['website'] = website_link['href']\n",
    "                    \n",
    "                    for link in soup.find_all('a', href=True):\n",
    "                        href = link['href']\n",
    "                        if 'facebook.com' in href:\n",
    "                            details['social_media']['facebook'] = href\n",
    "                        elif 'instagram.com' in href:\n",
    "                            details['social_media']['instagram'] = href\n",
    "                    \n",
    "                    # Additional info\n",
    "                    if info_h2 := soup.find('h2', string=lambda x: x and 'Additional Information' in x):\n",
    "                        if container := info_h2.parent.parent.find_next_sibling('div'):\n",
    "                            for field_div in container.find_all('div', style=lambda x: x and 'padding-bottom: 8px; margin-left: 15px;' in x):\n",
    "                                if label_div := field_div.find('div', style='font-weight: bold;'):\n",
    "                                    if label := label_div.strong:\n",
    "                                        divs = field_div.find_all('div', recursive=False)\n",
    "                                        if len(divs) >= 2:\n",
    "                                            if value_div := divs[1].find('div'):\n",
    "                                                details['additional_info'][label.text.strip()] = value_div.text.strip()\n",
    "                    \n",
    "                    rso.update(details)\n",
    "                    logger.info(f\"Processed: {rso.get('name')}\")\n",
    "                    return rso\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {url}: {e}\")\n",
    "                return rso\n",
    "\n",
    "    async def scrape_all_rsos(self):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            tasks = [\n",
    "                self.scrape_detail_page(session, rso['full_url'], rso)\n",
    "                for rso in rsos if 'full_url' in rso\n",
    "            ]\n",
    "            \n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            valid_results = [r for r in results if isinstance(r, dict)]\n",
    "            \n",
    "            with open('rso_data_detailed.json', 'w') as f:\n",
    "                json.dump(valid_results, f, indent=2)\n",
    "\n",
    "    async def test_scraping(self, num_test_pages=3):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)[:num_test_pages]\n",
    "            \n",
    "            logger.info(f\"Testing {num_test_pages} RSO pages...\")\n",
    "            \n",
    "            for rso in rsos:\n",
    "                if url := rso.get('full_url'):\n",
    "                    logger.info(f\"\\nTesting: {rso.get('name', 'Unknown')}\")\n",
    "                    logger.info(f\"URL: {url}\")\n",
    "                    \n",
    "                    result = await self.scrape_detail_page(session, url, rso.copy())\n",
    "                    if result:\n",
    "                        logger.info(\"\\nScraped Details:\")\n",
    "                        logger.info(f\"Description: {result['full_description'][:200]}...\")\n",
    "                        logger.info(f\"Contact: {result['contact']}\")\n",
    "                        logger.info(f\"Social Media: {result.get('social_media', {})}\")\n",
    "                        logger.info(f\"Additional Info: {result['additional_info']}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Testing 3 RSO pages...\n",
      "INFO:__main__:\n",
      "Testing: A Cappella Council\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/acacouncil\n",
      "ERROR:__main__:Error processing https://blueprint.uchicago.edu/organization/acacouncil: Cannot connect to host blueprint.uchicago.edu:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')]\n",
      "INFO:__main__:\n",
      "Scraped Details:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'full_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m scraper \u001b[38;5;241m=\u001b[39m AsyncRSOScraper()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39mtest_scraping()\n",
      "Cell \u001b[0;32mIn[6], line 93\u001b[0m, in \u001b[0;36mAsyncRSOScraper.test_scraping\u001b[0;34m(self, num_test_pages)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     92\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraped Details:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContact: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontact\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocial Media: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocial_media\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m{})\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_description'"
     ]
    }
   ],
   "source": [
    "scraper = AsyncRSOScraper()\n",
    "\n",
    "\n",
    "await scraper.test_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
