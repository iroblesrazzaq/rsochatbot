{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json'):\n",
    "        self.input_file = input_file\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing extra whitespace and asterisks\"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        return text.strip('* ').strip()\n",
    "    \n",
    "    def scrape_detail_page(self, url):\n",
    "        \"\"\"Scrape a single RSO detail page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(1)  # Allow page to load\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            details = {\n",
    "                'full_description': '',\n",
    "                'contact': {},\n",
    "                'additional_info': {}\n",
    "            }\n",
    "            \n",
    "            # Get full description from the correct div class and all paragraph tags within\n",
    "            description_div = soup.find('div', class_='bodyText-large userSupplied')\n",
    "            if description_div:\n",
    "                paragraphs = description_div.find_all('p')\n",
    "                details['full_description'] = ' '.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            \n",
    "            # Get contact information - updated for new structure\n",
    "            contact_email_div = soup.find('span', class_='sr-only', string='Contact Email')\n",
    "            if contact_email_div and contact_email_div.parent:\n",
    "                email_text = contact_email_div.parent.get_text()\n",
    "                # Extract the email from the text (everything after \"E: \")\n",
    "                if 'E:' in email_text:\n",
    "                    email = email_text.split('E:')[1].strip().strip('\"')\n",
    "                    details['contact']['email'] = email\n",
    "                \n",
    "            # Get address if available\n",
    "            address_div = soup.find('div', string='Address')\n",
    "            if address_div and address_div.find_next('div'):\n",
    "                details['contact']['address'] = self.clean_text(address_div.find_next('div').get_text())\n",
    "            \n",
    "            \n",
    "            # Get social media links and website\n",
    "            social_media = {}\n",
    "            \n",
    "            # Get website from aria-label\n",
    "            website_link = soup.find('a', attrs={'aria-label': lambda x: x and 'Visit our site' in x})\n",
    "            if website_link and website_link.get('href'):\n",
    "                social_media['website'] = website_link['href']\n",
    "            \n",
    "            # Get social media links\n",
    "            social_links = soup.find_all('a', href=True)\n",
    "            for link in social_links:\n",
    "                href = link['href']\n",
    "                if 'facebook.com' in href:\n",
    "                    social_media['facebook'] = href\n",
    "                elif 'instagram.com' in href:\n",
    "                    social_media['instagram'] = href\n",
    "            \n",
    "            if social_media:\n",
    "                details['social_media'] = social_media\n",
    "\n",
    "\n",
    "           # Get additional information\n",
    "            additional_info_h2 = soup.find('h2', string=lambda x: x and 'Additional Information' in x)\n",
    "            if additional_info_h2:\n",
    "                # Get to the container div\n",
    "                container = additional_info_h2.parent.parent.find_next_sibling('div')\n",
    "                if container:\n",
    "                    # Find all field divs by their specific style\n",
    "                    field_divs = container.find_all('div', style=lambda x: x and 'padding-bottom: 8px; margin-left: 15px;' in x)\n",
    "                    \n",
    "                    for field_div in field_divs:\n",
    "                        # Get the label from the strong tag\n",
    "                        label_div = field_div.find('div', style='font-weight: bold;')\n",
    "                        if label_div and label_div.strong:\n",
    "                            label = label_div.strong.text.strip()\n",
    "                            \n",
    "                            # Get the value by getting the second div (skipping the label div)\n",
    "                            divs = field_div.find_all('div', recursive=False)\n",
    "                            if len(divs) >= 2:  # Make sure we have both label and value divs\n",
    "                                value_div = divs[1].find('div')  # Get the inner div of the second div\n",
    "                                if value_div:\n",
    "                                    value = value_div.text.strip()\n",
    "                                    details['additional_info'][label] = value\n",
    "                    \n",
    "                    logger.info(f\"Extracted additional info: {details['additional_info']}\")\n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(rsos)} RSOs from {self.input_file}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i, rso in enumerate(rsos):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save updated data to new file\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "    \n",
    "\n",
    "\n",
    "    # test method\n",
    "    def test_detail_scraping(self, num_test_pages=3):\n",
    "        \"\"\"Test the detail scraping on a few RSO pages and print results\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Running test on {num_test_pages} RSO pages...\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            for i, rso in enumerate(rsos[:num_test_pages]):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"\\nTesting RSO {i+1}: {rso.get('name', 'Unknown')}\")\n",
    "                logger.info(f\"URL: {url}\")\n",
    "                \n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                # Print detailed results\n",
    "                if details:\n",
    "                    logger.info(\"\\nScraped Details:\")\n",
    "                    logger.info(f\"Description: {details['full_description'][:200]}...\")\n",
    "                    logger.info(f\"Contact Info: {details['contact']}\")\n",
    "                    logger.info(f\"Social Media: {details.get('social_media', {})}\")\n",
    "                    logger.info(f\"Additional Info: {details['additional_info']}\")\n",
    "                else:\n",
    "                    logger.error(\"Failed to scrape details\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in test process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RSODetailScraper()\n",
    "scraper.scrape_all_rsos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running test on 3 RSO pages...\n",
      "INFO:__main__:\n",
      "Testing RSO 1: A Cappella Council\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/acacouncil\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Amie Bernstein Clark', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2009', 'Regular Meetings (Day/Time/Location):': 'No Response', 'RSO Listhost:': 'acacouncil@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: A council comprised of representatives from all groups to oversee a cappella activities on campus. Organizes interactions between groups, event scheduling, microphone usage, and arbitration of the aud...\n",
      "INFO:__main__:Contact Info: {'email': 'uchicagoacappella@gmail.com', 'address': 'Contact Email E:  uchicagoacappella@gmail.com'}\n",
      "INFO:__main__:Social Media: {'website': 'http://uchicagoacappella.org', 'instagram': 'https://www.instagram.com/uchicagoacacouncil/', 'facebook': 'https://www.facebook.com/uchicagoacappella'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Amie Bernstein Clark', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2009', 'Regular Meetings (Day/Time/Location):': 'No Response', 'RSO Listhost:': 'acacouncil@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Testing RSO 2: Active Minds at the University of Chicago\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/active-minds\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2007', 'Regular Meetings (Day/Time/Location):': 'Tuesdays at 7:00pm in Harper 150', 'RSO Listhost:': 'active_minds@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'Yes', 'Parent Organization Name and Website:': 'Active Minds\\nhttp://www.activeminds.org/'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: The national purpose of Active Minds is to empower university students to speak openly about mental health in order to educate others and encourage help-seeking behavior. The purpose of Active Minds o...\n",
      "INFO:__main__:Contact Info: {'email': 'activemindsuchi@gmail.com'}\n",
      "INFO:__main__:Social Media: {'website': 'http://voices.uchicago.edu/activeminds', 'instagram': 'https://www.instagram.com/activemindsuchicago/', 'facebook': 'https://www.facebook.com/uchicagoactiveminds/?fref=ts'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2007', 'Regular Meetings (Day/Time/Location):': 'Tuesdays at 7:00pm in Harper 150', 'RSO Listhost:': 'active_minds@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'Yes', 'Parent Organization Name and Website:': 'Active Minds\\nhttp://www.activeminds.org/'}\n",
      "INFO:__main__:\n",
      "Testing RSO 3: African and Caribbean Students Association\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/acsa\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Purple Group', 'Year Created:': '2004', 'Regular Meetings (Day/Time/Location):': 'Sundays', 'RSO Listhost:': 'acsa@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: ACSA aims to build a vibrant cultural community for African and Caribbean students at the University of Chicago. Our mission is to provide a space where students can deepen their understanding of the ...\n",
      "INFO:__main__:Contact Info: {'email': 'zuricofer@uchicago.edu'}\n",
      "INFO:__main__:Social Media: {'instagram': 'https://www.instagram.com/acsa.uchicago/', 'facebook': 'https://www.facebook.com/UChicagoACSA'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Purple Group', 'Year Created:': '2004', 'Regular Meetings (Day/Time/Location):': 'Sundays', 'RSO Listhost:': 'acsa@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n"
     ]
    }
   ],
   "source": [
    "scraper = RSODetailScraper()\n",
    "# Run test first\n",
    "scraper.test_detail_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
