{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json', \n",
    "                 checkpoint_file='rso_scraping_checkpoint.json',\n",
    "                 checkpoint_frequency=10):  # Save every 10 RSOs\n",
    "        self.input_file = input_file\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        self.driver = None\n",
    "        \n",
    "    def save_checkpoint(self, rsos, last_processed_index):\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'last_processed_index': last_processed_index,\n",
    "            'rsos': rsos\n",
    "        }\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        logger.info(f\"Checkpoint saved at index {last_processed_index}\")\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load progress from checkpoint file if it exists\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                checkpoint_data = json.load(f)\n",
    "            logger.info(f\"Resuming from checkpoint at index {checkpoint_data['last_processed_index']}\")\n",
    "            return checkpoint_data['last_processed_index'], checkpoint_data['rsos']\n",
    "        except FileNotFoundError:\n",
    "            logger.info(\"No checkpoint found, starting from beginning\")\n",
    "            return -1, None\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing extra whitespace and asterisks\"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        return text.strip('* ').strip()\n",
    "    \n",
    "    def scrape_detail_page(self, url):\n",
    "        \"\"\"Scrape a single RSO detail page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(1)  # Allow page to load\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            details = {\n",
    "                'full_description': '',\n",
    "                'contact': {},\n",
    "                'additional_info': {}\n",
    "            }\n",
    "            \n",
    "            # Get full description from the correct div class and all paragraph tags within\n",
    "            description_div = soup.find('div', class_='bodyText-large userSupplied')\n",
    "            if description_div:\n",
    "                paragraphs = description_div.find_all('p')\n",
    "                details['full_description'] = ' '.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            \n",
    "            # Get contact information - updated for new structure\n",
    "            contact_email_div = soup.find('span', class_='sr-only', string='Contact Email')\n",
    "            if contact_email_div and contact_email_div.parent:\n",
    "                email_text = contact_email_div.parent.get_text()\n",
    "                # Extract the email from the text (everything after \"E: \")\n",
    "                if 'E:' in email_text:\n",
    "                    email = email_text.split('E:')[1].strip().strip('\"')\n",
    "                    details['contact']['email'] = email\n",
    "                \n",
    "            # Get address if available\n",
    "            address_div = soup.find('div', string='Address')\n",
    "            if address_div and address_div.find_next('div'):\n",
    "                details['contact']['address'] = self.clean_text(address_div.find_next('div').get_text())\n",
    "            \n",
    "            \n",
    "            # Get social media links and website\n",
    "            social_media = {}\n",
    "            \n",
    "            # Get website from aria-label\n",
    "            website_link = soup.find('a', attrs={'aria-label': lambda x: x and 'Visit our site' in x})\n",
    "            if website_link and website_link.get('href'):\n",
    "                social_media['website'] = website_link['href']\n",
    "            \n",
    "            # Get social media links\n",
    "            social_links = soup.find_all('a', href=True)\n",
    "            for link in social_links:\n",
    "                href = link['href']\n",
    "                if 'facebook.com' in href:\n",
    "                    social_media['facebook'] = href\n",
    "                elif 'instagram.com' in href:\n",
    "                    social_media['instagram'] = href\n",
    "            \n",
    "            if social_media:\n",
    "                details['social_media'] = social_media\n",
    "\n",
    "\n",
    "           # Get additional information\n",
    "            additional_info_h2 = soup.find('h2', string=lambda x: x and 'Additional Information' in x)\n",
    "            if additional_info_h2:\n",
    "                # Get to the container div\n",
    "                container = additional_info_h2.parent.parent.find_next_sibling('div')\n",
    "                if container:\n",
    "                    # Find all field divs by their specific style\n",
    "                    field_divs = container.find_all('div', style=lambda x: x and 'padding-bottom: 8px; margin-left: 15px;' in x)\n",
    "                    \n",
    "                    for field_div in field_divs:\n",
    "                        # Get the label from the strong tag\n",
    "                        label_div = field_div.find('div', style='font-weight: bold;')\n",
    "                        if label_div and label_div.strong:\n",
    "                            label = label_div.strong.text.strip()\n",
    "                            \n",
    "                            # Get the value by getting the second div (skipping the label div)\n",
    "                            divs = field_div.find_all('div', recursive=False)\n",
    "                            if len(divs) >= 2:  # Make sure we have both label and value divs\n",
    "                                value_div = divs[1].find('div')  # Get the inner div of the second div\n",
    "                                if value_div:\n",
    "                                    value = value_div.text.strip()\n",
    "                                    details['additional_info'][label] = value\n",
    "                    \n",
    "                    logger.info(f\"Extracted additional info: {details['additional_info']}\")\n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "            \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs with checkpointing\"\"\"\n",
    "        try:\n",
    "            # Try to load from checkpoint first\n",
    "            last_processed_index, checkpoint_rsos = self.load_checkpoint()\n",
    "            \n",
    "            # If no checkpoint, load from input file\n",
    "            if checkpoint_rsos is None:\n",
    "                with open(self.input_file, 'r') as f:\n",
    "                    rsos = json.load(f)\n",
    "                start_index = 0\n",
    "            else:\n",
    "                rsos = checkpoint_rsos\n",
    "                start_index = last_processed_index + 1\n",
    "            \n",
    "            logger.info(f\"Processing {len(rsos)} RSOs starting from index {start_index}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i in range(start_index, len(rsos)):\n",
    "                rso = rsos[i]\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if (i + 1) % self.checkpoint_frequency == 0:\n",
    "                    self.save_checkpoint(rsos, i)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save final results\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            # Clean up checkpoint file after successful completion\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "                \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "            # Save checkpoint on error\n",
    "            if 'rsos' in locals() and 'i' in locals():\n",
    "                self.save_checkpoint(rsos, i)\n",
    "                logger.info(\"Progress saved to checkpoint file after error\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json'):\n",
    "        self.input_file = input_file\n",
    "        self.driver = None\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(rsos)} RSOs from {self.input_file}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i, rso in enumerate(rsos):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save updated data to new file\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "    \n",
    "\n",
    "\n",
    "    # test method\n",
    "    def test_detail_scraping(self, num_test_pages=3):\n",
    "        \"\"\"Test the detail scraping on a few RSO pages and print results\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Running test on {num_test_pages} RSO pages...\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            for i, rso in enumerate(rsos[:num_test_pages]):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"\\nTesting RSO {i+1}: {rso.get('name', 'Unknown')}\")\n",
    "                logger.info(f\"URL: {url}\")\n",
    "                \n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                # Print detailed results\n",
    "                if details:\n",
    "                    logger.info(\"\\nScraped Details:\")\n",
    "                    logger.info(f\"Description: {details['full_description'][:200]}...\")\n",
    "                    logger.info(f\"Contact Info: {details['contact']}\")\n",
    "                    logger.info(f\"Social Media: {details.get('social_media', {})}\")\n",
    "                    logger.info(f\"Additional Info: {details['additional_info']}\")\n",
    "                else:\n",
    "                    logger.error(\"Failed to scrape details\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in test process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 408 RSOs from rso_data_with_categories.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scraper \u001b[38;5;241m=\u001b[39m RSODetailScraper()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_all_rsos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m, in \u001b[0;36mRSODetailScraper.scrape_all_rsos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     rsos \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    115\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rsos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RSOs from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Process each RSO\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rso \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rsos):\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mRSODetailScraper.setup_driver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--disable-dev-shm-usage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--window-size=1920,1080\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs_projects/venv1/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs_projects/venv1/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/cs_projects/venv1/lib/python3.11/site-packages/selenium/webdriver/common/driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[0;34m(service, options)\u001b[0m\n\u001b[1;32m     36\u001b[0m path \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mSeleniumManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/cs_projects/venv1/lib/python3.11/site-packages/selenium/webdriver/common/selenium_manager.py:103\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    100\u001b[0m     value \u001b[38;5;241m=\u001b[39m proxy\u001b[38;5;241m.\u001b[39mssl_proxy \u001b[38;5;28;01mif\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mssl_proxy \u001b[38;5;28;01melse\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mhttp_proxy\n\u001b[1;32m    101\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m browser_path \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    106\u001b[0m driver_path \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/cs_projects/venv1/lib/python3.11/site-packages/selenium/webdriver/common/selenium_manager.py:134\u001b[0m, in \u001b[0;36mSeleniumManager.run\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    132\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(args, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, creationflags\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mCREATE_NO_WINDOW)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     completed_proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m stdout \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m stderr \u001b[38;5;241m=\u001b[39m completed_proc\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper = RSODetailScraper()\n",
    "scraper.scrape_all_rsos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running test on 3 RSO pages...\n",
      "INFO:__main__:\n",
      "Testing RSO 1: A Cappella Council\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/acacouncil\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Amie Bernstein Clark', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2009', 'Regular Meetings (Day/Time/Location):': 'No Response', 'RSO Listhost:': 'acacouncil@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: A council comprised of representatives from all groups to oversee a cappella activities on campus. Organizes interactions between groups, event scheduling, microphone usage, and arbitration of the aud...\n",
      "INFO:__main__:Contact Info: {'email': 'uchicagoacappella@gmail.com', 'address': 'Contact Email E:  uchicagoacappella@gmail.com'}\n",
      "INFO:__main__:Social Media: {'website': 'http://uchicagoacappella.org', 'instagram': 'https://www.instagram.com/uchicagoacacouncil/', 'facebook': 'https://www.facebook.com/uchicagoacappella'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Amie Bernstein Clark', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2009', 'Regular Meetings (Day/Time/Location):': 'No Response', 'RSO Listhost:': 'acacouncil@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Testing RSO 2: Active Minds at the University of Chicago\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/active-minds\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2007', 'Regular Meetings (Day/Time/Location):': 'Tuesdays at 7:00pm in Harper 150', 'RSO Listhost:': 'active_minds@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'Yes', 'Parent Organization Name and Website:': 'Active Minds\\nhttp://www.activeminds.org/'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: The national purpose of Active Minds is to empower university students to speak openly about mental health in order to educate others and encourage help-seeking behavior. The purpose of Active Minds o...\n",
      "INFO:__main__:Contact Info: {'email': 'activemindsuchi@gmail.com'}\n",
      "INFO:__main__:Social Media: {'website': 'http://voices.uchicago.edu/activeminds', 'instagram': 'https://www.instagram.com/activemindsuchicago/', 'facebook': 'https://www.facebook.com/uchicagoactiveminds/?fref=ts'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Green Group', 'Year Created:': '2007', 'Regular Meetings (Day/Time/Location):': 'Tuesdays at 7:00pm in Harper 150', 'RSO Listhost:': 'active_minds@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'Yes', 'Parent Organization Name and Website:': 'Active Minds\\nhttp://www.activeminds.org/'}\n",
      "INFO:__main__:\n",
      "Testing RSO 3: African and Caribbean Students Association\n",
      "INFO:__main__:URL: https://blueprint.uchicago.edu/organization/acsa\n",
      "INFO:__main__:Extracted additional info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Purple Group', 'Year Created:': '2004', 'Regular Meetings (Day/Time/Location):': 'Sundays', 'RSO Listhost:': 'acsa@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n",
      "INFO:__main__:\n",
      "Scraped Details:\n",
      "INFO:__main__:Description: ACSA aims to build a vibrant cultural community for African and Caribbean students at the University of Chicago. Our mission is to provide a space where students can deepen their understanding of the ...\n",
      "INFO:__main__:Contact Info: {'email': 'zuricofer@uchicago.edu'}\n",
      "INFO:__main__:Social Media: {'instagram': 'https://www.instagram.com/acsa.uchicago/', 'facebook': 'https://www.facebook.com/UChicagoACSA'}\n",
      "INFO:__main__:Additional Info: {'RSO Advisor': 'Lauren Harris', 'Advising Model Categorization:': 'Purple Group', 'Year Created:': '2004', 'Regular Meetings (Day/Time/Location):': 'Sundays', 'RSO Listhost:': 'acsa@lists.uchicago.edu', 'This organization is affiliated with a parent/national/international organization.': 'No Response', 'Parent Organization Name and Website:': 'No Response'}\n"
     ]
    }
   ],
   "source": [
    "scraper = RSODetailScraper()\n",
    "# Run test first\n",
    "scraper.test_detail_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
