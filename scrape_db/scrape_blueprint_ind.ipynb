{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json', \n",
    "                 checkpoint_file='rso_scraping_checkpoint.json',\n",
    "                 checkpoint_frequency=10):  # Save every 10 RSOs\n",
    "        self.input_file = input_file\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        self.driver = None\n",
    "        \n",
    "    def save_checkpoint(self, rsos, last_processed_index):\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'last_processed_index': last_processed_index,\n",
    "            'rsos': rsos\n",
    "        }\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        logger.info(f\"Checkpoint saved at index {last_processed_index}\")\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load progress from checkpoint file if it exists\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                checkpoint_data = json.load(f)\n",
    "            logger.info(f\"Resuming from checkpoint at index {checkpoint_data['last_processed_index']}\")\n",
    "            return checkpoint_data['last_processed_index'], checkpoint_data['rsos']\n",
    "        except FileNotFoundError:\n",
    "            logger.info(\"No checkpoint found, starting from beginning\")\n",
    "            return -1, None\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing extra whitespace and asterisks\"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        return text.strip('* ').strip()\n",
    "    \n",
    "    def scrape_detail_page(self, url):\n",
    "        \"\"\"Scrape a single RSO detail page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(1)  # Allow page to load\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            details = {\n",
    "                'full_description': '',\n",
    "                'contact': {},\n",
    "                'additional_info': {}\n",
    "            }\n",
    "            \n",
    "            # Get full description from the correct div class and all paragraph tags within\n",
    "            description_div = soup.find('div', class_='bodyText-large userSupplied')\n",
    "            if description_div:\n",
    "                paragraphs = description_div.find_all('p')\n",
    "                details['full_description'] = ' '.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            \n",
    "            # Get contact information - updated for new structure\n",
    "            contact_email_div = soup.find('span', class_='sr-only', string='Contact Email')\n",
    "            if contact_email_div and contact_email_div.parent:\n",
    "                email_text = contact_email_div.parent.get_text()\n",
    "                # Extract the email from the text (everything after \"E: \")\n",
    "                if 'E:' in email_text:\n",
    "                    email = email_text.split('E:')[1].strip().strip('\"')\n",
    "                    details['contact']['email'] = email\n",
    "                \n",
    "            # Get address if available\n",
    "            address_div = soup.find('div', string='Address')\n",
    "            if address_div and address_div.find_next('div'):\n",
    "                details['contact']['address'] = self.clean_text(address_div.find_next('div').get_text())\n",
    "            \n",
    "            \n",
    "            # Get social media links and website\n",
    "            social_media = {}\n",
    "            \n",
    "            # Get website from aria-label\n",
    "            website_link = soup.find('a', attrs={'aria-label': lambda x: x and 'Visit our site' in x})\n",
    "            if website_link and website_link.get('href'):\n",
    "                social_media['website'] = website_link['href']\n",
    "            \n",
    "            # Get social media links\n",
    "            social_links = soup.find_all('a', href=True)\n",
    "            for link in social_links:\n",
    "                href = link['href']\n",
    "                if 'facebook.com' in href:\n",
    "                    social_media['facebook'] = href\n",
    "                elif 'instagram.com' in href:\n",
    "                    social_media['instagram'] = href\n",
    "            \n",
    "            if social_media:\n",
    "                details['social_media'] = social_media\n",
    "\n",
    "\n",
    "           # Get additional information\n",
    "            additional_info_h2 = soup.find('h2', string=lambda x: x and 'Additional Information' in x)\n",
    "            if additional_info_h2:\n",
    "                # Get to the container div\n",
    "                container = additional_info_h2.parent.parent.find_next_sibling('div')\n",
    "                if container:\n",
    "                    # Find all field divs by their specific style\n",
    "                    field_divs = container.find_all('div', style=lambda x: x and 'padding-bottom: 8px; margin-left: 15px;' in x)\n",
    "                    \n",
    "                    for field_div in field_divs:\n",
    "                        # Get the label from the strong tag\n",
    "                        label_div = field_div.find('div', style='font-weight: bold;')\n",
    "                        if label_div and label_div.strong:\n",
    "                            label = label_div.strong.text.strip()\n",
    "                            \n",
    "                            # Get the value by getting the second div (skipping the label div)\n",
    "                            divs = field_div.find_all('div', recursive=False)\n",
    "                            if len(divs) >= 2:  # Make sure we have both label and value divs\n",
    "                                value_div = divs[1].find('div')  # Get the inner div of the second div\n",
    "                                if value_div:\n",
    "                                    value = value_div.text.strip()\n",
    "                                    details['additional_info'][label] = value\n",
    "                    \n",
    "                    logger.info(f\"Extracted additional info: {details['additional_info']}\")\n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "            \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs with checkpointing\"\"\"\n",
    "        try:\n",
    "            # Try to load from checkpoint first\n",
    "            last_processed_index, checkpoint_rsos = self.load_checkpoint()\n",
    "            \n",
    "            # If no checkpoint, load from input file\n",
    "            if checkpoint_rsos is None:\n",
    "                with open(self.input_file, 'r') as f:\n",
    "                    rsos = json.load(f)\n",
    "                start_index = 0\n",
    "            else:\n",
    "                rsos = checkpoint_rsos\n",
    "                start_index = last_processed_index + 1\n",
    "            \n",
    "            logger.info(f\"Processing {len(rsos)} RSOs starting from index {start_index}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i in range(start_index, len(rsos)):\n",
    "                rso = rsos[i]\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if (i + 1) % self.checkpoint_frequency == 0:\n",
    "                    self.save_checkpoint(rsos, i)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save final results\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            # Clean up checkpoint file after successful completion\n",
    "            if os.path.exists(self.checkpoint_file):\n",
    "                os.remove(self.checkpoint_file)\n",
    "                \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "            # Save checkpoint on error\n",
    "            if 'rsos' in locals() and 'i' in locals():\n",
    "                self.save_checkpoint(rsos, i)\n",
    "                logger.info(\"Progress saved to checkpoint file after error\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSODetailScraper:\n",
    "    def __init__(self, input_file='rso_data_with_categories.json'):\n",
    "        self.input_file = input_file\n",
    "        self.driver = None\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    def scrape_all_rsos(self):\n",
    "        \"\"\"Scrape details for all RSOs\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(rsos)} RSOs from {self.input_file}\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            # Process each RSO\n",
    "            for i, rso in enumerate(rsos):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Processing RSO {i+1}/{len(rsos)}: {rso.get('name', 'Unknown')}\")\n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                if details:\n",
    "                    rso.update(details)\n",
    "                \n",
    "                # Add a small delay between requests\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Save updated data to new file\n",
    "            output_file = 'rso_data_detailed.json'\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(rsos, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Successfully saved detailed RSO data to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in scraping process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "    \n",
    "\n",
    "\n",
    "    # test method\n",
    "    def test_detail_scraping(self, num_test_pages=3):\n",
    "        \"\"\"Test the detail scraping on a few RSO pages and print results\"\"\"\n",
    "        try:\n",
    "            # Load existing RSO data\n",
    "            with open(self.input_file, 'r') as f:\n",
    "                rsos = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Running test on {num_test_pages} RSO pages...\")\n",
    "            self.setup_driver()\n",
    "            \n",
    "            for i, rso in enumerate(rsos[:num_test_pages]):\n",
    "                url = rso.get('full_url')\n",
    "                if not url:\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"\\nTesting RSO {i+1}: {rso.get('name', 'Unknown')}\")\n",
    "                logger.info(f\"URL: {url}\")\n",
    "                \n",
    "                details = self.scrape_detail_page(url)\n",
    "                \n",
    "                # Print detailed results\n",
    "                if details:\n",
    "                    logger.info(\"\\nScraped Details:\")\n",
    "                    logger.info(f\"Description: {details['full_description'][:200]}...\")\n",
    "                    logger.info(f\"Contact Info: {details['contact']}\")\n",
    "                    logger.info(f\"Social Media: {details.get('social_media', {})}\")\n",
    "                    logger.info(f\"Additional Info: {details['additional_info']}\")\n",
    "                else:\n",
    "                    logger.error(\"Failed to scrape details\")\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in test process: {str(e)}\")\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RSODetailScraper()\n",
    "scraper.scrape_all_rsos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RSODetailScraper()\n",
    "# Run test first\n",
    "scraper.test_detail_scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
