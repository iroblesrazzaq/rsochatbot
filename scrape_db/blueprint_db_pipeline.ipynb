{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "INDEX_NAME = \"rso-chatbot\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def safe_get(dictionary: Dict, key: str, default: Any = \"None\") -> Any:\n",
    "    \"\"\"Safely get a value from a dictionary, returning default if None or missing.\"\"\"\n",
    "    value = dictionary.get(key)\n",
    "    return default if value is None else value\n",
    "\n",
    "def generate_safe_id(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a safe ASCII ID from a string.\n",
    "    Handles unicode characters, spaces, and special characters.\n",
    "    \"\"\"\n",
    "    if not name or name.lower() == \"none\":\n",
    "        return f\"unknown-rso-{hash(str(name))}\"\n",
    "        \n",
    "    # Convert to ASCII, remove diacritics\n",
    "    normalized = unicodedata.normalize('NFKD', name)\n",
    "    ascii_name = normalized.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Replace special characters and spaces with hyphens\n",
    "    safe_id = re.sub(r'[^a-zA-Z0-9]+', '-', ascii_name.lower())\n",
    "    \n",
    "    # Remove leading/trailing hyphens\n",
    "    safe_id = safe_id.strip('-')\n",
    "    \n",
    "    # Ensure we have a valid ID\n",
    "    if not safe_id:\n",
    "        return f\"unnamed-rso-{hash(name)}\"\n",
    "        \n",
    "    return safe_id\n",
    "\n",
    "def transform_rso_data(rso_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Transform RSO data for Pinecone database with safe None handling.\"\"\"\n",
    "    # Handle None case for entire RSO\n",
    "    if rso_data is None:\n",
    "        return {\n",
    "            \"name\": \"None\",\n",
    "            \"full_url\": \"None\",\n",
    "            \"description\": \"None\",\n",
    "            \"categories\": [],\n",
    "            \"contact_email\": \"None\",\n",
    "            \"additional_info\": {},\n",
    "            \"social_media_links\": []\n",
    "        }\n",
    "    \n",
    "    # Extract high confidence AI categories with None handling\n",
    "    ai_categories = safe_get(rso_data, \"ai_categories\", [])\n",
    "    high_confidence_categories = [\n",
    "        cat[\"name\"] \n",
    "        for cat in ai_categories \n",
    "        if cat.get(\"confidence\", 0) >= 85 and cat.get(\"name\")\n",
    "    ]\n",
    "    \n",
    "    # Combine with original categories, remove duplicates, handle None\n",
    "    original_categories = safe_get(rso_data, \"categories\", [])\n",
    "    all_categories = list(set(\n",
    "        [cat for cat in (original_categories + high_confidence_categories) if cat]\n",
    "    ))\n",
    "    \n",
    "    # Use full_description if available, otherwise fall back to description_preview\n",
    "    description = (\n",
    "        safe_get(rso_data, \"full_description\") \n",
    "        if rso_data.get(\"full_description\")\n",
    "        else safe_get(rso_data, \"description_preview\")\n",
    "    )\n",
    "    \n",
    "    # Safely get contact information\n",
    "    contact = safe_get(rso_data, \"contact\", {})\n",
    "    contact_email = safe_get(contact, \"email\") if isinstance(contact, dict) else \"None\"\n",
    "    \n",
    "    # Safely get and flatten additional_info\n",
    "    additional_info = safe_get(rso_data, \"additional_info\", {})\n",
    "    flattened_additional_info = {\n",
    "        str(k): str(v) if v is not None else \"None\"\n",
    "        for k, v in additional_info.items()\n",
    "    } if isinstance(additional_info, dict) else {}\n",
    "    \n",
    "    # Safely get social media links\n",
    "    social_media = safe_get(rso_data, \"social_media\", {})\n",
    "    social_media_links = [\n",
    "        str(link) for link in social_media.values()\n",
    "        if link is not None\n",
    "    ] if isinstance(social_media, dict) else []\n",
    "    \n",
    "    # Create transformed dictionary with only desired fields and proper types\n",
    "    transformed_data = {\n",
    "        \"name\": safe_get(rso_data, \"name\"),\n",
    "        \"full_url\": safe_get(rso_data, \"full_url\"),\n",
    "        \"description\": description,\n",
    "        \"categories\": all_categories,\n",
    "        \"contact_email\": contact_email,\n",
    "        \"additional_info\": flattened_additional_info,\n",
    "        \"social_media_links\": social_media_links\n",
    "    }\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def generate_embedding(rso_data: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Generate embedding for RSO data.\"\"\"\n",
    "    # Combine relevant text fields for embedding\n",
    "    text_to_embed = f\"{rso_data['name']} {rso_data['description']} {' '.join(rso_data['categories'])}\"\n",
    "    \n",
    "    # Generate embedding\n",
    "    embedding = model.encode(text_to_embed)\n",
    "    return embedding\n",
    "\n",
    "def prepare_pinecone_data(rso_data: Dict[str, Any], embedding: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Prepare data for Pinecone upsert.\"\"\"\n",
    "    # Generate safe ASCII ID\n",
    "    safe_id = generate_safe_id(rso_data[\"name\"])\n",
    "    \n",
    "    return {\n",
    "        \"id\": safe_id,\n",
    "        \"values\": embedding.tolist(),\n",
    "        \"metadata\": {\n",
    "            \"name\": rso_data[\"name\"],\n",
    "            \"full_url\": rso_data[\"full_url\"],\n",
    "            \"description\": rso_data[\"description\"],\n",
    "            \"categories\": rso_data[\"categories\"],\n",
    "            \"contact_email\": rso_data[\"contact_email\"],\n",
    "            \"social_media_links\": rso_data[\"social_media_links\"],\n",
    "            \"additional_info\": [f\"{k}: {v}\" for k, v in rso_data[\"additional_info\"].items()]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_all_rsos(data, batch_size=100):\n",
    "    \"\"\"Process all RSOs and upsert to Pinecone in batches.\"\"\"\n",
    "    # Ensure data is a list\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    vectors = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, rso in enumerate(data):\n",
    "        try:\n",
    "            transformed_data = transform_rso_data(rso)\n",
    "            embedding = generate_embedding(transformed_data)\n",
    "            vector = prepare_pinecone_data(transformed_data, embedding)\n",
    "            vectors.append(vector)\n",
    "            successful += 1\n",
    "            \n",
    "            # Upsert when batch is full\n",
    "            if len(vectors) >= batch_size:\n",
    "                index.upsert(vectors=vectors)\n",
    "                print(f\"Upserted batch of {len(vectors)} vectors\")\n",
    "                vectors = []\n",
    "                \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i + 1} RSOs (Successful: {successful}, Failed: {failed})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"Error processing RSO {rso.get('name', 'unknown')}: {str(e)}\")\n",
    "    \n",
    "    # Upsert any remaining vectors\n",
    "    if vectors:\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Upserted final batch of {len(vectors)} vectors\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Total RSOs processed: {len(data)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 RSOs (Successful: 10, Failed: 0)\n",
      "Processed 20 RSOs (Successful: 20, Failed: 0)\n",
      "Processed 30 RSOs (Successful: 30, Failed: 0)\n",
      "Processed 40 RSOs (Successful: 40, Failed: 0)\n",
      "Processed 50 RSOs (Successful: 50, Failed: 0)\n",
      "Processed 60 RSOs (Successful: 60, Failed: 0)\n",
      "Processed 70 RSOs (Successful: 70, Failed: 0)\n",
      "Processed 80 RSOs (Successful: 80, Failed: 0)\n",
      "Processed 90 RSOs (Successful: 90, Failed: 0)\n",
      "Upserted batch of 100 vectors\n",
      "Processed 100 RSOs (Successful: 100, Failed: 0)\n",
      "Processed 110 RSOs (Successful: 110, Failed: 0)\n",
      "Processed 120 RSOs (Successful: 120, Failed: 0)\n",
      "Processed 130 RSOs (Successful: 130, Failed: 0)\n",
      "Processed 140 RSOs (Successful: 140, Failed: 0)\n",
      "Processed 150 RSOs (Successful: 150, Failed: 0)\n",
      "Processed 160 RSOs (Successful: 160, Failed: 0)\n",
      "Processed 170 RSOs (Successful: 170, Failed: 0)\n",
      "Processed 180 RSOs (Successful: 180, Failed: 0)\n",
      "Processed 190 RSOs (Successful: 190, Failed: 0)\n",
      "Upserted batch of 100 vectors\n",
      "Processed 200 RSOs (Successful: 200, Failed: 0)\n",
      "Processed 210 RSOs (Successful: 210, Failed: 0)\n",
      "Processed 220 RSOs (Successful: 220, Failed: 0)\n",
      "Processed 230 RSOs (Successful: 230, Failed: 0)\n",
      "Processed 240 RSOs (Successful: 240, Failed: 0)\n",
      "Processed 250 RSOs (Successful: 250, Failed: 0)\n",
      "Processed 260 RSOs (Successful: 260, Failed: 0)\n",
      "Processed 270 RSOs (Successful: 270, Failed: 0)\n",
      "Processed 280 RSOs (Successful: 280, Failed: 0)\n",
      "Processed 290 RSOs (Successful: 290, Failed: 0)\n",
      "Upserted batch of 100 vectors\n",
      "Processed 300 RSOs (Successful: 300, Failed: 0)\n",
      "Processed 310 RSOs (Successful: 310, Failed: 0)\n",
      "Processed 320 RSOs (Successful: 320, Failed: 0)\n",
      "Processed 330 RSOs (Successful: 330, Failed: 0)\n",
      "Processed 340 RSOs (Successful: 340, Failed: 0)\n",
      "Processed 350 RSOs (Successful: 350, Failed: 0)\n",
      "Processed 360 RSOs (Successful: 360, Failed: 0)\n",
      "Processed 370 RSOs (Successful: 370, Failed: 0)\n",
      "Processed 380 RSOs (Successful: 380, Failed: 0)\n",
      "Processed 390 RSOs (Successful: 390, Failed: 0)\n",
      "Upserted batch of 100 vectors\n",
      "Processed 400 RSOs (Successful: 400, Failed: 0)\n",
      "Upserted final batch of 8 vectors\n",
      "\n",
      "Final Results:\n",
      "Total RSOs processed: 408\n",
      "Successful: 408\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "process_all_rsos(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
